{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "from scipy.sparse import csgraph\n",
    "import sys\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from utils import *\n",
    "from graphConvolution import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STEP 1]: Upload cora dataset.\n",
      "| # of nodes : 2708\n",
      "| # of edges : 5278.0\n",
      "| # of features : 1433\n",
      "| # of clases   : 7\n",
      "| # of train set : 140\n",
      "| # of val set   : 500\n",
      "| # of test set  : 1000\n"
     ]
    }
   ],
   "source": [
    "adj, features, labels, idx_train, idx_val, idx_test = load_data(dataset='cora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "features, adj, labels = Variable(features), Variable(adj), Variable(labels)\n",
    "# torch.cuda.manual_seed(72)\n",
    "features = features.cuda()\n",
    "adj = adj.cuda()\n",
    "labels = labels.cuda()\n",
    "idx_train = idx_train.cuda()\n",
    "idx_val = idx_val.cuda()\n",
    "idx_test = idx_test.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FM-6layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolutionFM(Module):\n",
    "    def __init__(self, in_features, out_features, embedding, bias=True):\n",
    "        super(GraphConvolutionFM, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.embedding = embedding\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features),requires_grad=True)\n",
    "        self.V = Parameter(torch.randn(out_features, in_features, embedding),requires_grad=True)\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        self.V.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj,nhid1,nhid2,nhid3,nhid4):\n",
    "        out_lin = torch.mm(input, self.weight) + self.bias\n",
    "        # all\n",
    "        out_1 = torch.matmul(input,self.V).pow(2).sum(2, keepdim=True).view(self.out_features,input.shape[0]).t()\n",
    "        out_2 = torch.matmul(input.pow(2), self.V.pow(2)).sum(2, keepdim=True).view(self.out_features,input.shape[0]).t() \n",
    "        out_inter1 = 0.5*(out_1 - out_2)\n",
    "        # x1-xnhid1\n",
    "        out_3 = torch.matmul(input[:,:nhid1],self.V[:,:nhid1,:]).pow(2).sum(2, keepdim=True).view(self.out_features,input.shape[0]).t()\n",
    "        out_4 = torch.matmul(input[:,:nhid1].pow(2), self.V[:,:nhid1,:].pow(2)).sum(2, keepdim=True).view(self.out_features,input.shape[0]).t() \n",
    "        out_inter2 = 0.5*(out_3 - out_4)\n",
    "        # xnhid1-xnhid2\n",
    "        out_5 = torch.matmul(input[:,nhid1:nhid2],self.V[:,nhid1:nhid2,:]).pow(2).sum(2, keepdim=True).view(self.out_features,input.shape[0]).t()\n",
    "        out_6 = torch.matmul(input[:,nhid1:nhid2].pow(2), self.V[:,nhid1:nhid2,:].pow(2)).sum(2, keepdim=True).view(self.out_features,input.shape[0]).t() \n",
    "        out_inter3 = 0.5*(out_5 - out_6)\n",
    "        # xnhid2-xnhid3\n",
    "        out_7 = torch.matmul(input[:,nhid2:nhid3],self.V[:,nhid2:nhid3,:]).pow(2).sum(2, keepdim=True).view(self.out_features,input.shape[0]).t()\n",
    "        out_8 = torch.matmul(input[:,nhid2:nhid3].pow(2), self.V[:,nhid2:nhid3,:].pow(2)).sum(2, keepdim=True).view(self.out_features,input.shape[0]).t() \n",
    "        out_inter4 = 0.5*(out_7 - out_8)\n",
    "        # xnhid3-xnhid4\n",
    "        out_9 = torch.matmul(input[:,nhid3:nhid4],self.V[:,nhid3:nhid4,:]).pow(2).sum(2, keepdim=True).view(self.out_features,input.shape[0]).t()\n",
    "        out_10 = torch.matmul(input[:,nhid3:nhid4].pow(2), self.V[:,nhid3:nhid4,:].pow(2)).sum(2, keepdim=True).view(self.out_features,input.shape[0]).t() \n",
    "        out_inter5 = 0.5*(out_9- out_10)\n",
    "        \n",
    "        # xnhid4-xnhid5\n",
    "        out_11 = torch.matmul(input[:,nhid4:],self.V[:,nhid4:,:]).pow(2).sum(2, keepdim=True).view(self.out_features,input.shape[0]).t()\n",
    "        out_12 = torch.matmul(input[:,nhid4:].pow(2), self.V[:,nhid4:,:].pow(2)).sum(2, keepdim=True).view(self.out_features,input.shape[0]).t() \n",
    "        out_inter6 = 0.5*(out_11- out_12)\n",
    "        \n",
    "        out_inter = out_inter1 - out_inter2 - out_inter3- out_inter4- out_inter5-out_inter6\n",
    "        \n",
    "        output = out_inter + out_lin\n",
    "        \n",
    "        output = torch.spmm(adj, output) \n",
    "        return output\n",
    "    \n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid1,nhid2, nhid3,nhid4,nhid5,nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        self.nhid1 = nhid1\n",
    "        self.nhid2 = nhid2\n",
    "        self.nhid3 = nhid3\n",
    "        self.nhid4 = nhid4\n",
    "        self.nhid5 = nhid5\n",
    "        \n",
    "        self.gc_1 = GraphConvolution(nfeat, nhid1,bias=True)\n",
    "        self.gc_2 = GraphConvolution(nfeat, nhid2,bias=True)\n",
    "        self.gc_3 = GraphConvolution(nfeat, nhid3,bias=True)\n",
    "        self.gc_4 = GraphConvolution(nfeat, nhid4,bias=True)\n",
    "        \n",
    "        self.gc1 = GraphConvolution(nfeat, nhid1,bias=True)\n",
    "        self.gc1_2 = GraphConvolution(nhid1, nhid2,bias=True)\n",
    "        self.gc1_3 = GraphConvolution(nhid1, nhid3,bias=True)\n",
    "        self.gc1_4 = GraphConvolution(nhid1, nhid4,bias=True)\n",
    "        \n",
    "        \n",
    "        self.gc2 = GraphConvolution(nhid1, nhid2,bias=True)\n",
    "        self.gc2_3 = GraphConvolution(nhid2, nhid3,bias=True)\n",
    "        self.gc2_4 = GraphConvolution(nhid2, nhid4,bias=True)\n",
    "        \n",
    "        self.W1 = nn.Parameter(torch.ones(2708, 2),requires_grad=True)\n",
    "        self.W2 = nn.Parameter(torch.ones(2708, 3),requires_grad=True)\n",
    "        self.W3 = nn.Parameter(torch.ones(2708, 4),requires_grad=True)\n",
    "        self.W4 = nn.Parameter(torch.ones(2708, 5),requires_grad=True)\n",
    "        \n",
    "        self.gc3 = GraphConvolution(nhid2, nhid3,bias=True)\n",
    "        self.gc3_4 = GraphConvolution(nhid3, nhid4,bias=True)\n",
    "        \n",
    "        self.gc4 = GraphConvolution(nhid3, nhid4,bias=True)\n",
    "        \n",
    "        self.gc5 = GraphConvolution(nhid4, nhid5,bias=True)\n",
    "        \n",
    "        self.gcFM6 = GraphConvolutionFM(nhid5+nhid4+nhid3+nhid2+nhid1, nclass,embedding=5,bias=True)\n",
    "        \n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        W1 = torch.exp(self.W1)\n",
    "        W1 = W1/torch.max(W1,dim=1).values.view(-1,1)\n",
    "        W1 = torch.bernoulli(W1)\n",
    "        \n",
    "        W2 = torch.exp(self.W2)\n",
    "        W2 = W2/torch.max(W2,dim=1).values.view(-1,1)\n",
    "        W2 = torch.bernoulli(W2)\n",
    "        \n",
    "        W3 = torch.exp(self.W3)\n",
    "        W3 = W3/torch.max(W3,dim=1).values.view(-1,1)\n",
    "        W3 = torch.bernoulli(W3)\n",
    "        \n",
    "        W4 = torch.exp(self.W4)\n",
    "        W4 = W4/torch.max(W4,dim=1).values.view(-1,1)\n",
    "        W4 = torch.bernoulli(W4)\n",
    "        \n",
    "        x_d = F.dropout(x, self.dropout, training=self.training)\n",
    "        x_df1 = F.dropout(F.relu(self.gc_1(x_d,adj)), self.dropout, training=self.training)\n",
    "        x_df2 = F.dropout(F.relu(self.gc_2(x_d,adj)), self.dropout, training=self.training)\n",
    "        x_df3 = F.dropout(F.relu(self.gc_3(x_d,adj)), self.dropout, training=self.training)\n",
    "        x_df4 = F.dropout(F.relu(self.gc_4(x_d,adj)), self.dropout, training=self.training)\n",
    "\n",
    "        x1_d = F.dropout(F.relu(self.gc1(x_d, adj)), training=self.training)\n",
    "        x1_df2 = F.dropout(F.relu(self.gc1_2(x1_d,adj)), self.dropout, training=self.training)\n",
    "        x1_df3 = F.dropout(F.relu(self.gc1_3(x1_d,adj)), self.dropout, training=self.training)\n",
    "        x1_df4 = F.dropout(F.relu(self.gc1_4(x1_d,adj)), self.dropout, training=self.training)\n",
    "        \n",
    "        combined1 =  torch.mul(x_df1, self.W1[:,0].view(2708,1)) + torch.mul(x1_d,self.W1[:,1].view(2708,1))\n",
    "        combined1 = F.dropout(combined1, self.dropout, training=self.training)\n",
    "        \n",
    "        x2_d = F.dropout(F.relu(self.gc2(combined1, adj)), self.dropout, training=self.training)\n",
    "        x2_df3 = F.dropout(F.relu(self.gc2_3(x2_d,adj)), self.dropout, training=self.training)\n",
    "        x2_df4 = F.dropout(F.relu(self.gc2_4(x2_d,adj)), self.dropout, training=self.training)\n",
    "\n",
    "        combined2 =  torch.mul(x_df2, self.W2[:,0].view(2708,1)) + torch.mul(x1_df2, self.W2[:,0].view(2708,1))+ torch.mul(x2_d,self.W2[:,2].view(2708,1))\n",
    "        combined2 = F.dropout(combined2, self.dropout, training=self.training)\n",
    "\n",
    "        x3_d = F.dropout(F.relu(self.gc3(combined2, adj)), self.dropout, training=self.training)\n",
    "        x3_df4 = F.dropout(F.relu(self.gc3_4(x3_d,adj)), self.dropout, training=self.training)\n",
    "        \n",
    "        combined3 =  torch.mul(x_df3, self.W3[:,0].view(2708,1)) +torch.mul(x1_df3, self.W3[:,1].view(2708,1))+torch.mul(x2_df3, self.W3[:,2].view(2708,1))+ torch.mul(x3_d,self.W3[:,3].view(2708,1)) \n",
    "        combined3 = F.dropout(combined3, self.dropout, training=self.training)\n",
    "        \n",
    "        x4_d = F.dropout(F.relu(self.gc4(combined3, adj)), self.dropout, training=self.training)\n",
    "        \n",
    "        combined4 =  torch.mul(x_df4, self.W4[:,0].view(2708,1)) +torch.mul(x1_df4, self.W4[:,1].view(2708,1))+torch.mul(x2_df4, self.W4[:,2].view(2708,1))+torch.mul(x3_df4, self.W4[:,3].view(2708,1))+torch.mul(x4_d,self.W4[:,4].view(2708,1))  \n",
    "        combined4 = F.dropout(combined4, self.dropout, training=self.training)\n",
    "        \n",
    "        \n",
    "        x5_d = F.dropout(F.relu(self.gc5(combined4, adj)), self.dropout, training=self.training)\n",
    "        \n",
    "        combined5 = torch.cat([x1_d, x2_d, x3_d, x4_d, x5_d], dim=1)\n",
    "        combined5 = F.dropout(combined5, self.dropout, training=self.training)\n",
    "        \n",
    "        x6 = self.gcFM6(combined5,adj,self.nhid1,self.nhid2+self.nhid1,self.nhid3+self.nhid2+self.nhid1,self.nhid4+self.nhid3+self.nhid2+self.nhid1) \n",
    "        return F.log_softmax(x6, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model,record):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = F.cross_entropy(output[idx_train], labels[idx_train]) \n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "\n",
    "    loss_val = F.cross_entropy(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    \n",
    "    loss_test = F.cross_entropy(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'acc_test: {:.4f}'.format(acc_test.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "    record[acc_val.item()] = acc_test.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 10.5455 acc_train: 0.1571 acc_val: 0.1580 acc_test: 0.1460 time: 0.4238s\n",
      "Epoch: 0002 loss_train: 4.1885 acc_train: 0.0786 acc_val: 0.1620 acc_test: 0.1490 time: 0.0331s\n",
      "Epoch: 0003 loss_train: 2.9610 acc_train: 0.1357 acc_val: 0.1620 acc_test: 0.1490 time: 0.0334s\n",
      "Epoch: 0004 loss_train: 2.1248 acc_train: 0.1929 acc_val: 0.1620 acc_test: 0.1490 time: 0.0353s\n",
      "Epoch: 0005 loss_train: 2.5547 acc_train: 0.0786 acc_val: 0.1620 acc_test: 0.1490 time: 0.0353s\n",
      "Epoch: 0006 loss_train: 3.1888 acc_train: 0.1357 acc_val: 0.1680 acc_test: 0.1480 time: 0.0350s\n",
      "Epoch: 0007 loss_train: 2.0081 acc_train: 0.1000 acc_val: 0.1720 acc_test: 0.1510 time: 0.0342s\n",
      "Epoch: 0008 loss_train: 2.0028 acc_train: 0.1643 acc_val: 0.1900 acc_test: 0.1990 time: 0.0318s\n",
      "Epoch: 0009 loss_train: 1.9744 acc_train: 0.1071 acc_val: 0.1700 acc_test: 0.1860 time: 0.0271s\n",
      "Epoch: 0010 loss_train: 1.9905 acc_train: 0.1286 acc_val: 0.1540 acc_test: 0.1770 time: 0.0316s\n",
      "Epoch: 0011 loss_train: 1.9695 acc_train: 0.1500 acc_val: 0.1500 acc_test: 0.1650 time: 0.0336s\n",
      "Epoch: 0012 loss_train: 2.0243 acc_train: 0.1643 acc_val: 0.1360 acc_test: 0.1480 time: 0.0365s\n",
      "Epoch: 0013 loss_train: 1.9985 acc_train: 0.2143 acc_val: 0.1280 acc_test: 0.1330 time: 0.0327s\n",
      "Epoch: 0014 loss_train: 2.1157 acc_train: 0.1429 acc_val: 0.1220 acc_test: 0.1290 time: 0.0337s\n",
      "Epoch: 0015 loss_train: 1.9674 acc_train: 0.1429 acc_val: 0.1240 acc_test: 0.1290 time: 0.0358s\n",
      "Epoch: 0016 loss_train: 1.9405 acc_train: 0.1929 acc_val: 0.1240 acc_test: 0.1290 time: 0.0319s\n",
      "Epoch: 0017 loss_train: 1.9437 acc_train: 0.1500 acc_val: 0.1240 acc_test: 0.1290 time: 0.0268s\n",
      "Epoch: 0018 loss_train: 1.9402 acc_train: 0.1857 acc_val: 0.1220 acc_test: 0.1290 time: 0.0316s\n",
      "Epoch: 0019 loss_train: 1.9371 acc_train: 0.1643 acc_val: 0.1220 acc_test: 0.1290 time: 0.0358s\n",
      "Epoch: 0020 loss_train: 1.9379 acc_train: 0.1857 acc_val: 0.1220 acc_test: 0.1290 time: 0.0352s\n",
      "Epoch: 0021 loss_train: 1.9355 acc_train: 0.1643 acc_val: 0.1220 acc_test: 0.1290 time: 0.0318s\n",
      "Epoch: 0022 loss_train: 1.9181 acc_train: 0.1929 acc_val: 0.1240 acc_test: 0.1310 time: 0.0262s\n",
      "Epoch: 0023 loss_train: 1.9321 acc_train: 0.1857 acc_val: 0.1260 acc_test: 0.1320 time: 0.0315s\n",
      "Epoch: 0024 loss_train: 1.9312 acc_train: 0.1929 acc_val: 0.1280 acc_test: 0.1360 time: 0.0337s\n",
      "Epoch: 0025 loss_train: 1.8975 acc_train: 0.1571 acc_val: 0.1300 acc_test: 0.1400 time: 0.0369s\n",
      "Epoch: 0026 loss_train: 1.9114 acc_train: 0.2143 acc_val: 0.1300 acc_test: 0.1410 time: 0.0321s\n",
      "Epoch: 0027 loss_train: 1.8978 acc_train: 0.2286 acc_val: 0.1400 acc_test: 0.1500 time: 0.0345s\n",
      "Epoch: 0028 loss_train: 1.9127 acc_train: 0.2143 acc_val: 0.1560 acc_test: 0.1670 time: 0.0331s\n",
      "Epoch: 0029 loss_train: 1.9347 acc_train: 0.2143 acc_val: 0.1560 acc_test: 0.1790 time: 0.0266s\n",
      "Epoch: 0030 loss_train: 1.9457 acc_train: 0.1643 acc_val: 0.1740 acc_test: 0.1900 time: 0.0332s\n",
      "Epoch: 0031 loss_train: 1.8998 acc_train: 0.2571 acc_val: 0.1920 acc_test: 0.2030 time: 0.0347s\n",
      "Epoch: 0032 loss_train: 1.8946 acc_train: 0.2857 acc_val: 0.2020 acc_test: 0.2130 time: 0.0336s\n",
      "Epoch: 0033 loss_train: 1.9224 acc_train: 0.2214 acc_val: 0.2060 acc_test: 0.2170 time: 0.0342s\n",
      "Epoch: 0034 loss_train: 1.8879 acc_train: 0.2357 acc_val: 0.2120 acc_test: 0.2330 time: 0.0360s\n",
      "Epoch: 0035 loss_train: 1.8832 acc_train: 0.3000 acc_val: 0.2240 acc_test: 0.2430 time: 0.0321s\n",
      "Epoch: 0036 loss_train: 1.8897 acc_train: 0.2786 acc_val: 0.2460 acc_test: 0.2650 time: 0.0351s\n",
      "Epoch: 0037 loss_train: 1.8669 acc_train: 0.2571 acc_val: 0.2820 acc_test: 0.2830 time: 0.0365s\n",
      "Epoch: 0038 loss_train: 1.8902 acc_train: 0.3143 acc_val: 0.3100 acc_test: 0.3010 time: 0.0342s\n",
      "Epoch: 0039 loss_train: 1.8791 acc_train: 0.2571 acc_val: 0.3400 acc_test: 0.3260 time: 0.0342s\n",
      "Epoch: 0040 loss_train: 1.8687 acc_train: 0.2929 acc_val: 0.3600 acc_test: 0.3590 time: 0.0345s\n",
      "Epoch: 0041 loss_train: 1.8357 acc_train: 0.3071 acc_val: 0.3720 acc_test: 0.3770 time: 0.0345s\n",
      "Epoch: 0042 loss_train: 1.8378 acc_train: 0.3357 acc_val: 0.3920 acc_test: 0.3920 time: 0.0392s\n",
      "Epoch: 0043 loss_train: 1.8342 acc_train: 0.3286 acc_val: 0.4000 acc_test: 0.4010 time: 0.0317s\n",
      "Epoch: 0044 loss_train: 1.8521 acc_train: 0.2929 acc_val: 0.4040 acc_test: 0.4060 time: 0.0381s\n",
      "Epoch: 0045 loss_train: 1.8700 acc_train: 0.2643 acc_val: 0.3980 acc_test: 0.4130 time: 0.0412s\n",
      "Epoch: 0046 loss_train: 1.7930 acc_train: 0.4071 acc_val: 0.4040 acc_test: 0.4270 time: 0.0322s\n",
      "Epoch: 0047 loss_train: 1.7970 acc_train: 0.3857 acc_val: 0.4160 acc_test: 0.4370 time: 0.0342s\n",
      "Epoch: 0048 loss_train: 1.8309 acc_train: 0.3429 acc_val: 0.4400 acc_test: 0.4520 time: 0.0340s\n",
      "Epoch: 0049 loss_train: 1.8002 acc_train: 0.3786 acc_val: 0.4520 acc_test: 0.4770 time: 0.0342s\n",
      "Epoch: 0050 loss_train: 1.8020 acc_train: 0.4214 acc_val: 0.4780 acc_test: 0.5050 time: 0.0349s\n",
      "Epoch: 0051 loss_train: 1.8009 acc_train: 0.3143 acc_val: 0.5220 acc_test: 0.5440 time: 0.0342s\n",
      "Epoch: 0052 loss_train: 1.8085 acc_train: 0.3071 acc_val: 0.5660 acc_test: 0.5990 time: 0.0342s\n",
      "Epoch: 0053 loss_train: 1.7420 acc_train: 0.4214 acc_val: 0.6020 acc_test: 0.6430 time: 0.0343s\n",
      "Epoch: 0054 loss_train: 1.7559 acc_train: 0.4286 acc_val: 0.6300 acc_test: 0.6660 time: 0.0343s\n",
      "Epoch: 0055 loss_train: 1.7198 acc_train: 0.4429 acc_val: 0.6320 acc_test: 0.6700 time: 0.0353s\n",
      "Epoch: 0056 loss_train: 1.7360 acc_train: 0.4357 acc_val: 0.6100 acc_test: 0.6360 time: 0.0349s\n",
      "Epoch: 0057 loss_train: 1.7578 acc_train: 0.4000 acc_val: 0.6080 acc_test: 0.6150 time: 0.0342s\n",
      "Epoch: 0058 loss_train: 1.7557 acc_train: 0.4357 acc_val: 0.5980 acc_test: 0.6030 time: 0.0349s\n",
      "Epoch: 0059 loss_train: 1.7457 acc_train: 0.3714 acc_val: 0.5940 acc_test: 0.5900 time: 0.0333s\n",
      "Epoch: 0060 loss_train: 1.7325 acc_train: 0.4143 acc_val: 0.5840 acc_test: 0.5860 time: 0.0343s\n",
      "Epoch: 0061 loss_train: 1.6280 acc_train: 0.4571 acc_val: 0.5840 acc_test: 0.5840 time: 0.0340s\n",
      "Epoch: 0062 loss_train: 1.6916 acc_train: 0.4000 acc_val: 0.5820 acc_test: 0.5810 time: 0.0345s\n",
      "Epoch: 0063 loss_train: 1.7100 acc_train: 0.3929 acc_val: 0.5820 acc_test: 0.5830 time: 0.0342s\n",
      "Epoch: 0064 loss_train: 1.6317 acc_train: 0.4786 acc_val: 0.5880 acc_test: 0.5800 time: 0.0344s\n",
      "Epoch: 0065 loss_train: 1.6656 acc_train: 0.4000 acc_val: 0.5800 acc_test: 0.5730 time: 0.0343s\n",
      "Epoch: 0066 loss_train: 1.6395 acc_train: 0.4500 acc_val: 0.5740 acc_test: 0.5600 time: 0.0347s\n",
      "Epoch: 0067 loss_train: 1.6624 acc_train: 0.3929 acc_val: 0.5760 acc_test: 0.5620 time: 0.0353s\n",
      "Epoch: 0068 loss_train: 1.6534 acc_train: 0.4643 acc_val: 0.5900 acc_test: 0.5760 time: 0.0335s\n",
      "Epoch: 0069 loss_train: 1.6021 acc_train: 0.4000 acc_val: 0.6040 acc_test: 0.5940 time: 0.0345s\n",
      "Epoch: 0070 loss_train: 1.6680 acc_train: 0.4714 acc_val: 0.6180 acc_test: 0.6140 time: 0.0335s\n",
      "Epoch: 0071 loss_train: 1.5766 acc_train: 0.5000 acc_val: 0.6260 acc_test: 0.6270 time: 0.0349s\n",
      "Epoch: 0072 loss_train: 1.5975 acc_train: 0.4571 acc_val: 0.6360 acc_test: 0.6400 time: 0.0334s\n",
      "Epoch: 0073 loss_train: 1.6616 acc_train: 0.4071 acc_val: 0.6680 acc_test: 0.6820 time: 0.0351s\n",
      "Epoch: 0074 loss_train: 1.5090 acc_train: 0.5143 acc_val: 0.7120 acc_test: 0.7120 time: 0.0355s\n",
      "Epoch: 0075 loss_train: 1.5253 acc_train: 0.4714 acc_val: 0.7320 acc_test: 0.7310 time: 0.0332s\n",
      "Epoch: 0076 loss_train: 1.5686 acc_train: 0.4214 acc_val: 0.7380 acc_test: 0.7550 time: 0.0351s\n",
      "Epoch: 0077 loss_train: 1.5076 acc_train: 0.5214 acc_val: 0.7420 acc_test: 0.7620 time: 0.0331s\n",
      "Epoch: 0078 loss_train: 1.5187 acc_train: 0.4857 acc_val: 0.7460 acc_test: 0.7650 time: 0.0347s\n",
      "Epoch: 0079 loss_train: 1.4958 acc_train: 0.4857 acc_val: 0.7460 acc_test: 0.7590 time: 0.0338s\n",
      "Epoch: 0080 loss_train: 1.4527 acc_train: 0.5714 acc_val: 0.7460 acc_test: 0.7540 time: 0.0329s\n",
      "Epoch: 0081 loss_train: 1.5199 acc_train: 0.5143 acc_val: 0.7440 acc_test: 0.7540 time: 0.0347s\n",
      "Epoch: 0082 loss_train: 1.4555 acc_train: 0.5286 acc_val: 0.7380 acc_test: 0.7460 time: 0.0343s\n",
      "Epoch: 0083 loss_train: 1.4927 acc_train: 0.4429 acc_val: 0.7340 acc_test: 0.7370 time: 0.0343s\n",
      "Epoch: 0084 loss_train: 1.5024 acc_train: 0.4643 acc_val: 0.7220 acc_test: 0.7300 time: 0.0347s\n",
      "Epoch: 0085 loss_train: 1.3619 acc_train: 0.5286 acc_val: 0.7120 acc_test: 0.7210 time: 0.0337s\n",
      "Epoch: 0086 loss_train: 1.4292 acc_train: 0.5286 acc_val: 0.7120 acc_test: 0.7200 time: 0.0342s\n",
      "Epoch: 0087 loss_train: 1.4282 acc_train: 0.5214 acc_val: 0.7280 acc_test: 0.7300 time: 0.0342s\n",
      "Epoch: 0088 loss_train: 1.4522 acc_train: 0.5214 acc_val: 0.7420 acc_test: 0.7460 time: 0.0346s\n",
      "Epoch: 0089 loss_train: 1.4723 acc_train: 0.5000 acc_val: 0.7600 acc_test: 0.7630 time: 0.0331s\n",
      "Epoch: 0090 loss_train: 1.4397 acc_train: 0.5357 acc_val: 0.7720 acc_test: 0.7690 time: 0.0343s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0091 loss_train: 1.3442 acc_train: 0.5857 acc_val: 0.7820 acc_test: 0.7830 time: 0.0352s\n",
      "Epoch: 0092 loss_train: 1.3320 acc_train: 0.5714 acc_val: 0.7840 acc_test: 0.7890 time: 0.0332s\n",
      "Epoch: 0093 loss_train: 1.3480 acc_train: 0.5714 acc_val: 0.7860 acc_test: 0.7920 time: 0.0348s\n",
      "Epoch: 0094 loss_train: 1.3807 acc_train: 0.5357 acc_val: 0.7900 acc_test: 0.7900 time: 0.0343s\n",
      "Epoch: 0095 loss_train: 1.3459 acc_train: 0.5071 acc_val: 0.7880 acc_test: 0.7920 time: 0.0343s\n",
      "Epoch: 0096 loss_train: 1.2862 acc_train: 0.5929 acc_val: 0.7940 acc_test: 0.7970 time: 0.0341s\n",
      "Epoch: 0097 loss_train: 1.2845 acc_train: 0.5857 acc_val: 0.7980 acc_test: 0.8020 time: 0.0351s\n",
      "Epoch: 0098 loss_train: 1.3944 acc_train: 0.5714 acc_val: 0.7960 acc_test: 0.8060 time: 0.0333s\n",
      "Epoch: 0099 loss_train: 1.2081 acc_train: 0.5929 acc_val: 0.7920 acc_test: 0.8050 time: 0.0342s\n",
      "Epoch: 0100 loss_train: 1.2734 acc_train: 0.5857 acc_val: 0.7880 acc_test: 0.8020 time: 0.0342s\n",
      "Epoch: 0101 loss_train: 1.3263 acc_train: 0.6214 acc_val: 0.7860 acc_test: 0.8030 time: 0.0349s\n",
      "Epoch: 0102 loss_train: 1.2695 acc_train: 0.6000 acc_val: 0.7880 acc_test: 0.8020 time: 0.0341s\n",
      "Epoch: 0103 loss_train: 1.2325 acc_train: 0.5071 acc_val: 0.7920 acc_test: 0.7940 time: 0.0351s\n",
      "Epoch: 0104 loss_train: 1.3471 acc_train: 0.5214 acc_val: 0.7880 acc_test: 0.7880 time: 0.0332s\n",
      "Epoch: 0105 loss_train: 1.2296 acc_train: 0.6500 acc_val: 0.7800 acc_test: 0.7880 time: 0.0394s\n",
      "Epoch: 0106 loss_train: 1.2240 acc_train: 0.5786 acc_val: 0.7840 acc_test: 0.7870 time: 0.0318s\n",
      "Epoch: 0107 loss_train: 1.2606 acc_train: 0.5786 acc_val: 0.7780 acc_test: 0.7780 time: 0.0377s\n",
      "Epoch: 0108 loss_train: 1.2789 acc_train: 0.6143 acc_val: 0.7740 acc_test: 0.7790 time: 0.0378s\n",
      "Epoch: 0109 loss_train: 1.2652 acc_train: 0.6214 acc_val: 0.7700 acc_test: 0.7810 time: 0.0344s\n",
      "Epoch: 0110 loss_train: 1.2579 acc_train: 0.5857 acc_val: 0.7760 acc_test: 0.7780 time: 0.0343s\n",
      "Epoch: 0111 loss_train: 1.0688 acc_train: 0.6786 acc_val: 0.7800 acc_test: 0.7790 time: 0.0383s\n",
      "Epoch: 0112 loss_train: 1.1260 acc_train: 0.6071 acc_val: 0.7800 acc_test: 0.7780 time: 0.0330s\n",
      "Epoch: 0113 loss_train: 1.2319 acc_train: 0.5786 acc_val: 0.7800 acc_test: 0.7830 time: 0.0343s\n",
      "Epoch: 0114 loss_train: 1.1838 acc_train: 0.6714 acc_val: 0.7940 acc_test: 0.7950 time: 0.0343s\n",
      "Epoch: 0115 loss_train: 1.1214 acc_train: 0.6500 acc_val: 0.7960 acc_test: 0.8030 time: 0.0348s\n",
      "Epoch: 0116 loss_train: 1.0920 acc_train: 0.6500 acc_val: 0.8020 acc_test: 0.8090 time: 0.0343s\n",
      "Epoch: 0117 loss_train: 1.0904 acc_train: 0.7071 acc_val: 0.8060 acc_test: 0.8140 time: 0.0383s\n",
      "Epoch: 0118 loss_train: 1.1597 acc_train: 0.6143 acc_val: 0.8060 acc_test: 0.8250 time: 0.0353s\n",
      "Epoch: 0119 loss_train: 1.1199 acc_train: 0.6000 acc_val: 0.7940 acc_test: 0.8370 time: 0.0377s\n",
      "Epoch: 0120 loss_train: 1.1977 acc_train: 0.5929 acc_val: 0.7940 acc_test: 0.8360 time: 0.0349s\n",
      "Epoch: 0121 loss_train: 1.1394 acc_train: 0.6571 acc_val: 0.8000 acc_test: 0.8350 time: 0.0343s\n",
      "Epoch: 0122 loss_train: 1.1313 acc_train: 0.6143 acc_val: 0.8000 acc_test: 0.8330 time: 0.0341s\n",
      "Epoch: 0123 loss_train: 1.1643 acc_train: 0.6214 acc_val: 0.7960 acc_test: 0.8260 time: 0.0379s\n",
      "Epoch: 0124 loss_train: 1.0653 acc_train: 0.6357 acc_val: 0.7960 acc_test: 0.8180 time: 0.0334s\n",
      "Epoch: 0125 loss_train: 0.9906 acc_train: 0.7214 acc_val: 0.8000 acc_test: 0.8180 time: 0.0343s\n",
      "Epoch: 0126 loss_train: 1.0869 acc_train: 0.6571 acc_val: 0.7860 acc_test: 0.8000 time: 0.0347s\n",
      "Epoch: 0127 loss_train: 1.1140 acc_train: 0.6357 acc_val: 0.7800 acc_test: 0.7880 time: 0.0346s\n",
      "Epoch: 0128 loss_train: 1.1342 acc_train: 0.5929 acc_val: 0.7700 acc_test: 0.7820 time: 0.0331s\n",
      "Epoch: 0129 loss_train: 1.1133 acc_train: 0.6286 acc_val: 0.7660 acc_test: 0.7750 time: 0.0359s\n",
      "Epoch: 0130 loss_train: 1.0904 acc_train: 0.6714 acc_val: 0.7680 acc_test: 0.7730 time: 0.0333s\n",
      "Epoch: 0131 loss_train: 1.1242 acc_train: 0.6143 acc_val: 0.7600 acc_test: 0.7760 time: 0.0343s\n",
      "Epoch: 0132 loss_train: 1.1611 acc_train: 0.6000 acc_val: 0.7560 acc_test: 0.7720 time: 0.0340s\n",
      "Epoch: 0133 loss_train: 1.1187 acc_train: 0.6500 acc_val: 0.7580 acc_test: 0.7800 time: 0.0352s\n",
      "Epoch: 0134 loss_train: 1.1479 acc_train: 0.6143 acc_val: 0.7660 acc_test: 0.7880 time: 0.0328s\n",
      "Epoch: 0135 loss_train: 1.0118 acc_train: 0.6500 acc_val: 0.7880 acc_test: 0.7960 time: 0.0341s\n",
      "Epoch: 0136 loss_train: 1.1251 acc_train: 0.6143 acc_val: 0.8000 acc_test: 0.8100 time: 0.0342s\n",
      "Epoch: 0137 loss_train: 1.1119 acc_train: 0.6500 acc_val: 0.8040 acc_test: 0.8220 time: 0.0348s\n",
      "Epoch: 0138 loss_train: 1.0571 acc_train: 0.6286 acc_val: 0.8080 acc_test: 0.8330 time: 0.0342s\n",
      "Epoch: 0139 loss_train: 1.1440 acc_train: 0.6000 acc_val: 0.8060 acc_test: 0.8380 time: 0.0351s\n",
      "Epoch: 0140 loss_train: 1.1881 acc_train: 0.5643 acc_val: 0.8060 acc_test: 0.8370 time: 0.0332s\n",
      "Epoch: 0141 loss_train: 1.0929 acc_train: 0.5714 acc_val: 0.8060 acc_test: 0.8330 time: 0.0349s\n",
      "Epoch: 0142 loss_train: 1.0621 acc_train: 0.6286 acc_val: 0.8100 acc_test: 0.8310 time: 0.0331s\n",
      "Epoch: 0143 loss_train: 1.0378 acc_train: 0.6429 acc_val: 0.8100 acc_test: 0.8370 time: 0.0346s\n",
      "Epoch: 0144 loss_train: 0.9844 acc_train: 0.6500 acc_val: 0.8060 acc_test: 0.8390 time: 0.0343s\n",
      "Epoch: 0145 loss_train: 0.9576 acc_train: 0.7071 acc_val: 0.8040 acc_test: 0.8410 time: 0.0360s\n",
      "Epoch: 0146 loss_train: 1.0881 acc_train: 0.6500 acc_val: 0.8120 acc_test: 0.8420 time: 0.0330s\n",
      "Epoch: 0147 loss_train: 1.0998 acc_train: 0.6357 acc_val: 0.8080 acc_test: 0.8330 time: 0.0341s\n",
      "Epoch: 0148 loss_train: 1.0265 acc_train: 0.6143 acc_val: 0.8080 acc_test: 0.8310 time: 0.0343s\n",
      "Epoch: 0149 loss_train: 1.1242 acc_train: 0.6429 acc_val: 0.8100 acc_test: 0.8270 time: 0.0341s\n",
      "Epoch: 0150 loss_train: 0.9867 acc_train: 0.6500 acc_val: 0.8060 acc_test: 0.8260 time: 0.0349s\n",
      "Epoch: 0151 loss_train: 0.9917 acc_train: 0.6286 acc_val: 0.8020 acc_test: 0.8210 time: 0.0352s\n",
      "Epoch: 0152 loss_train: 0.9962 acc_train: 0.7071 acc_val: 0.7960 acc_test: 0.8170 time: 0.0331s\n",
      "Epoch: 0153 loss_train: 0.9252 acc_train: 0.7429 acc_val: 0.7940 acc_test: 0.8150 time: 0.0341s\n",
      "Epoch: 0154 loss_train: 1.1415 acc_train: 0.5929 acc_val: 0.7880 acc_test: 0.8130 time: 0.0344s\n",
      "Epoch: 0155 loss_train: 0.9367 acc_train: 0.7000 acc_val: 0.7900 acc_test: 0.8060 time: 0.0342s\n",
      "Epoch: 0156 loss_train: 0.9970 acc_train: 0.7214 acc_val: 0.7900 acc_test: 0.8070 time: 0.0342s\n",
      "Epoch: 0157 loss_train: 1.0618 acc_train: 0.6000 acc_val: 0.7920 acc_test: 0.8100 time: 0.0350s\n",
      "Epoch: 0158 loss_train: 1.1070 acc_train: 0.5500 acc_val: 0.7920 acc_test: 0.8080 time: 0.0331s\n",
      "Epoch: 0159 loss_train: 1.0256 acc_train: 0.6214 acc_val: 0.7920 acc_test: 0.8120 time: 0.0343s\n",
      "Epoch: 0160 loss_train: 1.0278 acc_train: 0.7071 acc_val: 0.7940 acc_test: 0.8140 time: 0.0347s\n",
      "Epoch: 0161 loss_train: 1.0506 acc_train: 0.6429 acc_val: 0.8020 acc_test: 0.8170 time: 0.0343s\n",
      "Epoch: 0162 loss_train: 1.0349 acc_train: 0.6714 acc_val: 0.8040 acc_test: 0.8160 time: 0.0343s\n",
      "Epoch: 0163 loss_train: 1.0098 acc_train: 0.6714 acc_val: 0.8120 acc_test: 0.8190 time: 0.0351s\n",
      "Epoch: 0164 loss_train: 1.0736 acc_train: 0.5929 acc_val: 0.8100 acc_test: 0.8280 time: 0.0340s\n",
      "Epoch: 0165 loss_train: 0.9439 acc_train: 0.7143 acc_val: 0.8040 acc_test: 0.8300 time: 0.0331s\n",
      "Epoch: 0166 loss_train: 1.0161 acc_train: 0.6143 acc_val: 0.8060 acc_test: 0.8320 time: 0.0349s\n",
      "Epoch: 0167 loss_train: 1.0109 acc_train: 0.6857 acc_val: 0.8120 acc_test: 0.8330 time: 0.0343s\n",
      "Epoch: 0168 loss_train: 1.0462 acc_train: 0.5929 acc_val: 0.8080 acc_test: 0.8350 time: 0.0341s\n",
      "Epoch: 0169 loss_train: 1.0162 acc_train: 0.6286 acc_val: 0.8120 acc_test: 0.8350 time: 0.0350s\n",
      "Epoch: 0170 loss_train: 1.0772 acc_train: 0.6571 acc_val: 0.8140 acc_test: 0.8320 time: 0.0331s\n",
      "Epoch: 0171 loss_train: 0.9439 acc_train: 0.6714 acc_val: 0.8060 acc_test: 0.8310 time: 0.0344s\n",
      "Epoch: 0172 loss_train: 0.9744 acc_train: 0.6786 acc_val: 0.8020 acc_test: 0.8270 time: 0.0342s\n",
      "Epoch: 0173 loss_train: 0.9335 acc_train: 0.7071 acc_val: 0.7920 acc_test: 0.8160 time: 0.0342s\n",
      "Epoch: 0174 loss_train: 1.0439 acc_train: 0.6500 acc_val: 0.7920 acc_test: 0.8040 time: 0.0343s\n",
      "Epoch: 0175 loss_train: 0.9282 acc_train: 0.6786 acc_val: 0.7940 acc_test: 0.8050 time: 0.0351s\n",
      "Epoch: 0176 loss_train: 1.0299 acc_train: 0.6643 acc_val: 0.7920 acc_test: 0.8050 time: 0.0358s\n",
      "Epoch: 0177 loss_train: 0.9400 acc_train: 0.6357 acc_val: 0.7920 acc_test: 0.8060 time: 0.0333s\n",
      "Epoch: 0178 loss_train: 0.9875 acc_train: 0.6786 acc_val: 0.7920 acc_test: 0.8050 time: 0.0354s\n",
      "Epoch: 0179 loss_train: 0.9974 acc_train: 0.7143 acc_val: 0.7980 acc_test: 0.8100 time: 0.0330s\n",
      "Epoch: 0180 loss_train: 0.9870 acc_train: 0.6214 acc_val: 0.8080 acc_test: 0.8170 time: 0.0351s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0181 loss_train: 1.0374 acc_train: 0.6643 acc_val: 0.8100 acc_test: 0.8280 time: 0.0340s\n",
      "Epoch: 0182 loss_train: 0.9312 acc_train: 0.7214 acc_val: 0.8100 acc_test: 0.8330 time: 0.0353s\n",
      "Epoch: 0183 loss_train: 0.9034 acc_train: 0.7429 acc_val: 0.8100 acc_test: 0.8370 time: 0.0323s\n",
      "Epoch: 0184 loss_train: 1.0272 acc_train: 0.6286 acc_val: 0.8100 acc_test: 0.8370 time: 0.0350s\n",
      "Epoch: 0185 loss_train: 0.9195 acc_train: 0.7000 acc_val: 0.8100 acc_test: 0.8370 time: 0.0324s\n",
      "Epoch: 0186 loss_train: 1.0081 acc_train: 0.6714 acc_val: 0.8120 acc_test: 0.8310 time: 0.0341s\n",
      "Epoch: 0187 loss_train: 1.0184 acc_train: 0.6357 acc_val: 0.8120 acc_test: 0.8240 time: 0.0350s\n",
      "Epoch: 0188 loss_train: 0.9599 acc_train: 0.6643 acc_val: 0.8040 acc_test: 0.8080 time: 0.0331s\n",
      "Epoch: 0189 loss_train: 1.0414 acc_train: 0.6214 acc_val: 0.7940 acc_test: 0.8050 time: 0.0342s\n",
      "Epoch: 0190 loss_train: 1.0277 acc_train: 0.6500 acc_val: 0.7900 acc_test: 0.7980 time: 0.0342s\n",
      "Epoch: 0191 loss_train: 1.0030 acc_train: 0.6929 acc_val: 0.7800 acc_test: 0.7940 time: 0.0342s\n",
      "Epoch: 0192 loss_train: 0.9283 acc_train: 0.7000 acc_val: 0.7700 acc_test: 0.7940 time: 0.0347s\n",
      "Epoch: 0193 loss_train: 0.8990 acc_train: 0.7571 acc_val: 0.7720 acc_test: 0.7880 time: 0.0350s\n",
      "Epoch: 0194 loss_train: 0.9870 acc_train: 0.6357 acc_val: 0.7720 acc_test: 0.7910 time: 0.0344s\n",
      "Epoch: 0195 loss_train: 0.9380 acc_train: 0.6929 acc_val: 0.7720 acc_test: 0.7960 time: 0.0343s\n",
      "Epoch: 0196 loss_train: 0.9785 acc_train: 0.6357 acc_val: 0.7800 acc_test: 0.8000 time: 0.0342s\n",
      "Epoch: 0197 loss_train: 0.9091 acc_train: 0.6857 acc_val: 0.7820 acc_test: 0.8000 time: 0.0343s\n",
      "Epoch: 0198 loss_train: 0.9330 acc_train: 0.6571 acc_val: 0.7860 acc_test: 0.8070 time: 0.0348s\n",
      "Epoch: 0199 loss_train: 1.0222 acc_train: 0.6143 acc_val: 0.7960 acc_test: 0.8110 time: 0.0339s\n",
      "Epoch: 0200 loss_train: 0.9366 acc_train: 0.7071 acc_val: 0.8060 acc_test: 0.8250 time: 0.0343s\n",
      "Epoch: 0201 loss_train: 1.0046 acc_train: 0.6357 acc_val: 0.8020 acc_test: 0.8320 time: 0.0343s\n",
      "Epoch: 0202 loss_train: 1.0681 acc_train: 0.6571 acc_val: 0.8060 acc_test: 0.8350 time: 0.0342s\n",
      "Epoch: 0203 loss_train: 0.9276 acc_train: 0.6929 acc_val: 0.8060 acc_test: 0.8380 time: 0.0329s\n",
      "Epoch: 0204 loss_train: 0.9731 acc_train: 0.6786 acc_val: 0.8100 acc_test: 0.8440 time: 0.0342s\n",
      "Epoch: 0205 loss_train: 0.9709 acc_train: 0.6857 acc_val: 0.8100 acc_test: 0.8380 time: 0.0350s\n",
      "Epoch: 0206 loss_train: 0.8480 acc_train: 0.7143 acc_val: 0.8160 acc_test: 0.8410 time: 0.0331s\n",
      "Epoch: 0207 loss_train: 0.9927 acc_train: 0.6357 acc_val: 0.8140 acc_test: 0.8370 time: 0.0341s\n",
      "Epoch: 0208 loss_train: 0.9433 acc_train: 0.7000 acc_val: 0.8100 acc_test: 0.8320 time: 0.0343s\n",
      "Epoch: 0209 loss_train: 1.0045 acc_train: 0.6786 acc_val: 0.8100 acc_test: 0.8320 time: 0.0344s\n",
      "Epoch: 0210 loss_train: 0.8171 acc_train: 0.7786 acc_val: 0.8120 acc_test: 0.8200 time: 0.0347s\n",
      "Epoch: 0211 loss_train: 0.8418 acc_train: 0.7500 acc_val: 0.8080 acc_test: 0.8140 time: 0.0350s\n",
      "Epoch: 0212 loss_train: 0.8669 acc_train: 0.7071 acc_val: 0.8020 acc_test: 0.8110 time: 0.0343s\n",
      "Epoch: 0213 loss_train: 0.8442 acc_train: 0.7214 acc_val: 0.7920 acc_test: 0.8070 time: 0.0349s\n",
      "Epoch: 0214 loss_train: 1.0061 acc_train: 0.6643 acc_val: 0.7900 acc_test: 0.8060 time: 0.0337s\n",
      "Epoch: 0215 loss_train: 0.8038 acc_train: 0.6857 acc_val: 0.7960 acc_test: 0.8060 time: 0.0350s\n",
      "Epoch: 0216 loss_train: 0.9004 acc_train: 0.6929 acc_val: 0.7920 acc_test: 0.8030 time: 0.0331s\n",
      "Epoch: 0217 loss_train: 0.8509 acc_train: 0.7000 acc_val: 0.7940 acc_test: 0.8030 time: 0.0364s\n",
      "Epoch: 0218 loss_train: 0.8216 acc_train: 0.7500 acc_val: 0.7940 acc_test: 0.8020 time: 0.0333s\n",
      "Epoch: 0219 loss_train: 0.8013 acc_train: 0.7214 acc_val: 0.7960 acc_test: 0.8070 time: 0.0322s\n",
      "Epoch: 0220 loss_train: 0.9111 acc_train: 0.7071 acc_val: 0.7980 acc_test: 0.8120 time: 0.0342s\n",
      "Epoch: 0221 loss_train: 0.9875 acc_train: 0.6786 acc_val: 0.8020 acc_test: 0.8170 time: 0.0344s\n",
      "Epoch: 0222 loss_train: 0.8309 acc_train: 0.7214 acc_val: 0.8080 acc_test: 0.8200 time: 0.0342s\n",
      "Epoch: 0223 loss_train: 0.8145 acc_train: 0.7429 acc_val: 0.8080 acc_test: 0.8210 time: 0.0354s\n",
      "Epoch: 0224 loss_train: 0.9255 acc_train: 0.6571 acc_val: 0.8100 acc_test: 0.8170 time: 0.0344s\n",
      "Epoch: 0225 loss_train: 0.8965 acc_train: 0.7000 acc_val: 0.8060 acc_test: 0.8120 time: 0.0343s\n",
      "Epoch: 0226 loss_train: 0.9151 acc_train: 0.7000 acc_val: 0.8060 acc_test: 0.8080 time: 0.0343s\n",
      "Epoch: 0227 loss_train: 0.9486 acc_train: 0.6571 acc_val: 0.8060 acc_test: 0.8100 time: 0.0354s\n",
      "Epoch: 0228 loss_train: 0.9234 acc_train: 0.7214 acc_val: 0.8040 acc_test: 0.8130 time: 0.0334s\n",
      "Epoch: 0229 loss_train: 0.6879 acc_train: 0.7929 acc_val: 0.8060 acc_test: 0.8150 time: 0.0353s\n",
      "Epoch: 0230 loss_train: 0.9182 acc_train: 0.6786 acc_val: 0.8040 acc_test: 0.8140 time: 0.0331s\n",
      "Epoch: 0231 loss_train: 0.7767 acc_train: 0.7000 acc_val: 0.8020 acc_test: 0.8190 time: 0.0343s\n",
      "Epoch: 0232 loss_train: 0.8275 acc_train: 0.7286 acc_val: 0.7960 acc_test: 0.8190 time: 0.0343s\n",
      "Epoch: 0233 loss_train: 0.8515 acc_train: 0.7714 acc_val: 0.7980 acc_test: 0.8160 time: 0.0332s\n",
      "Epoch: 0234 loss_train: 0.9130 acc_train: 0.7071 acc_val: 0.8060 acc_test: 0.8230 time: 0.0345s\n",
      "Epoch: 0235 loss_train: 0.8419 acc_train: 0.7143 acc_val: 0.8080 acc_test: 0.8230 time: 0.0350s\n",
      "Epoch: 0236 loss_train: 0.9035 acc_train: 0.7143 acc_val: 0.8060 acc_test: 0.8250 time: 0.0331s\n",
      "Epoch: 0237 loss_train: 0.8492 acc_train: 0.7429 acc_val: 0.8060 acc_test: 0.8250 time: 0.0340s\n",
      "Epoch: 0238 loss_train: 0.8879 acc_train: 0.7214 acc_val: 0.8080 acc_test: 0.8280 time: 0.0341s\n",
      "Epoch: 0239 loss_train: 0.8706 acc_train: 0.7429 acc_val: 0.8040 acc_test: 0.8290 time: 0.0352s\n",
      "Epoch: 0240 loss_train: 0.8458 acc_train: 0.7429 acc_val: 0.7980 acc_test: 0.8170 time: 0.0348s\n",
      "Epoch: 0241 loss_train: 0.7861 acc_train: 0.7429 acc_val: 0.8000 acc_test: 0.8130 time: 0.0353s\n",
      "Epoch: 0242 loss_train: 0.9093 acc_train: 0.7214 acc_val: 0.7900 acc_test: 0.8110 time: 0.0331s\n",
      "Epoch: 0243 loss_train: 0.8425 acc_train: 0.7357 acc_val: 0.7920 acc_test: 0.8090 time: 0.0341s\n",
      "Epoch: 0244 loss_train: 0.8430 acc_train: 0.6929 acc_val: 0.7920 acc_test: 0.8080 time: 0.0350s\n",
      "Epoch: 0245 loss_train: 0.9257 acc_train: 0.6571 acc_val: 0.7920 acc_test: 0.8130 time: 0.0348s\n",
      "Epoch: 0246 loss_train: 0.7662 acc_train: 0.7214 acc_val: 0.7820 acc_test: 0.8110 time: 0.0331s\n",
      "Epoch: 0247 loss_train: 0.7886 acc_train: 0.7357 acc_val: 0.7820 acc_test: 0.8100 time: 0.0361s\n",
      "Epoch: 0248 loss_train: 0.8408 acc_train: 0.7500 acc_val: 0.7820 acc_test: 0.8090 time: 0.0336s\n",
      "Epoch: 0249 loss_train: 0.7391 acc_train: 0.7500 acc_val: 0.7780 acc_test: 0.8120 time: 0.0354s\n",
      "Epoch: 0250 loss_train: 0.7448 acc_train: 0.7929 acc_val: 0.7840 acc_test: 0.8200 time: 0.0331s\n",
      "Epoch: 0251 loss_train: 0.8198 acc_train: 0.6929 acc_val: 0.7880 acc_test: 0.8230 time: 0.0354s\n",
      "Epoch: 0252 loss_train: 0.7719 acc_train: 0.8143 acc_val: 0.8000 acc_test: 0.8360 time: 0.0329s\n",
      "Epoch: 0253 loss_train: 0.7665 acc_train: 0.7429 acc_val: 0.8080 acc_test: 0.8380 time: 0.0338s\n",
      "Epoch: 0254 loss_train: 0.6113 acc_train: 0.7929 acc_val: 0.8100 acc_test: 0.8340 time: 0.0341s\n",
      "Epoch: 0255 loss_train: 0.7325 acc_train: 0.7643 acc_val: 0.8100 acc_test: 0.8390 time: 0.0334s\n",
      "Epoch: 0256 loss_train: 0.7957 acc_train: 0.7071 acc_val: 0.8020 acc_test: 0.8400 time: 0.0341s\n",
      "Epoch: 0257 loss_train: 0.7831 acc_train: 0.7286 acc_val: 0.7900 acc_test: 0.8330 time: 0.0355s\n",
      "Epoch: 0258 loss_train: 0.7474 acc_train: 0.7643 acc_val: 0.7920 acc_test: 0.8260 time: 0.0400s\n",
      "Epoch: 0259 loss_train: 0.6863 acc_train: 0.7929 acc_val: 0.7840 acc_test: 0.8170 time: 0.0390s\n",
      "Epoch: 0260 loss_train: 0.7774 acc_train: 0.7286 acc_val: 0.7840 acc_test: 0.8150 time: 0.0330s\n",
      "Epoch: 0261 loss_train: 0.7932 acc_train: 0.7357 acc_val: 0.7900 acc_test: 0.8200 time: 0.0340s\n",
      "Epoch: 0262 loss_train: 0.7994 acc_train: 0.7143 acc_val: 0.7920 acc_test: 0.8170 time: 0.0343s\n",
      "Epoch: 0263 loss_train: 0.6873 acc_train: 0.7714 acc_val: 0.7900 acc_test: 0.8190 time: 0.0342s\n",
      "Epoch: 0264 loss_train: 0.6856 acc_train: 0.7571 acc_val: 0.7940 acc_test: 0.8190 time: 0.0347s\n",
      "Epoch: 0265 loss_train: 0.7857 acc_train: 0.7214 acc_val: 0.7920 acc_test: 0.8170 time: 0.0350s\n",
      "Epoch: 0266 loss_train: 0.7255 acc_train: 0.7786 acc_val: 0.7940 acc_test: 0.8170 time: 0.0332s\n",
      "Epoch: 0267 loss_train: 0.8200 acc_train: 0.6929 acc_val: 0.7920 acc_test: 0.8180 time: 0.0325s\n",
      "Epoch: 0268 loss_train: 0.7678 acc_train: 0.7357 acc_val: 0.7880 acc_test: 0.8120 time: 0.0344s\n",
      "Epoch: 0269 loss_train: 0.7303 acc_train: 0.7714 acc_val: 0.7940 acc_test: 0.8100 time: 0.0344s\n",
      "Epoch: 0270 loss_train: 0.6800 acc_train: 0.8000 acc_val: 0.7920 acc_test: 0.8150 time: 0.0348s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0271 loss_train: 0.7178 acc_train: 0.7429 acc_val: 0.7940 acc_test: 0.8220 time: 0.0352s\n",
      "Epoch: 0272 loss_train: 0.6925 acc_train: 0.7643 acc_val: 0.7920 acc_test: 0.8210 time: 0.0333s\n",
      "Epoch: 0273 loss_train: 0.6320 acc_train: 0.8071 acc_val: 0.7860 acc_test: 0.8250 time: 0.0352s\n",
      "Epoch: 0274 loss_train: 0.6693 acc_train: 0.7571 acc_val: 0.7820 acc_test: 0.8220 time: 0.0342s\n",
      "Epoch: 0275 loss_train: 0.7556 acc_train: 0.7786 acc_val: 0.7880 acc_test: 0.8240 time: 0.0342s\n",
      "Epoch: 0276 loss_train: 0.7486 acc_train: 0.7500 acc_val: 0.7860 acc_test: 0.8210 time: 0.0321s\n",
      "Epoch: 0277 loss_train: 0.6563 acc_train: 0.8000 acc_val: 0.7840 acc_test: 0.8240 time: 0.0332s\n",
      "Epoch: 0278 loss_train: 0.7863 acc_train: 0.7286 acc_val: 0.7820 acc_test: 0.8170 time: 0.0343s\n",
      "Epoch: 0279 loss_train: 0.7109 acc_train: 0.7714 acc_val: 0.7840 acc_test: 0.8150 time: 0.0322s\n",
      "Epoch: 0280 loss_train: 0.5778 acc_train: 0.8214 acc_val: 0.7880 acc_test: 0.8100 time: 0.0322s\n",
      "Epoch: 0281 loss_train: 0.5611 acc_train: 0.8214 acc_val: 0.7840 acc_test: 0.8040 time: 0.0385s\n",
      "Epoch: 0282 loss_train: 0.6860 acc_train: 0.7714 acc_val: 0.7840 acc_test: 0.8040 time: 0.0324s\n",
      "Epoch: 0283 loss_train: 0.6729 acc_train: 0.8000 acc_val: 0.7840 acc_test: 0.8100 time: 0.0375s\n",
      "Epoch: 0284 loss_train: 0.6407 acc_train: 0.8357 acc_val: 0.7740 acc_test: 0.8170 time: 0.0387s\n",
      "Epoch: 0285 loss_train: 0.8367 acc_train: 0.7357 acc_val: 0.7740 acc_test: 0.8220 time: 0.0342s\n",
      "Epoch: 0286 loss_train: 0.6058 acc_train: 0.7786 acc_val: 0.7800 acc_test: 0.8240 time: 0.0380s\n",
      "Epoch: 0287 loss_train: 0.7171 acc_train: 0.7714 acc_val: 0.7740 acc_test: 0.8200 time: 0.0394s\n",
      "Epoch: 0288 loss_train: 0.5830 acc_train: 0.7857 acc_val: 0.7680 acc_test: 0.8140 time: 0.0344s\n",
      "Epoch: 0289 loss_train: 0.6893 acc_train: 0.7571 acc_val: 0.7600 acc_test: 0.8140 time: 0.0338s\n",
      "Epoch: 0290 loss_train: 0.5945 acc_train: 0.8286 acc_val: 0.7580 acc_test: 0.8120 time: 0.0349s\n",
      "Epoch: 0291 loss_train: 0.6088 acc_train: 0.8000 acc_val: 0.7540 acc_test: 0.8150 time: 0.0331s\n",
      "Epoch: 0292 loss_train: 0.7018 acc_train: 0.8143 acc_val: 0.7580 acc_test: 0.8160 time: 0.0342s\n",
      "Epoch: 0293 loss_train: 0.6726 acc_train: 0.7714 acc_val: 0.7580 acc_test: 0.8170 time: 0.0343s\n",
      "Epoch: 0294 loss_train: 0.7588 acc_train: 0.7643 acc_val: 0.7660 acc_test: 0.8200 time: 0.0349s\n",
      "Epoch: 0295 loss_train: 0.5574 acc_train: 0.8571 acc_val: 0.7640 acc_test: 0.8210 time: 0.0354s\n",
      "Epoch: 0296 loss_train: 0.5422 acc_train: 0.8071 acc_val: 0.7700 acc_test: 0.8130 time: 0.0321s\n",
      "Epoch: 0297 loss_train: 0.5809 acc_train: 0.8429 acc_val: 0.7720 acc_test: 0.8100 time: 0.0341s\n",
      "Epoch: 0298 loss_train: 0.6116 acc_train: 0.7714 acc_val: 0.7720 acc_test: 0.8080 time: 0.0342s\n",
      "Epoch: 0299 loss_train: 0.6693 acc_train: 0.7786 acc_val: 0.7720 acc_test: 0.8110 time: 0.0343s\n",
      "Epoch: 0300 loss_train: 0.5552 acc_train: 0.8143 acc_val: 0.7700 acc_test: 0.8090 time: 0.0341s\n",
      "Epoch: 0301 loss_train: 0.6778 acc_train: 0.7643 acc_val: 0.7600 acc_test: 0.8100 time: 0.0370s\n",
      "Epoch: 0302 loss_train: 0.7049 acc_train: 0.8286 acc_val: 0.7480 acc_test: 0.8090 time: 0.0330s\n",
      "Epoch: 0303 loss_train: 0.6703 acc_train: 0.7714 acc_val: 0.7520 acc_test: 0.8090 time: 0.0333s\n",
      "Epoch: 0304 loss_train: 0.6144 acc_train: 0.8143 acc_val: 0.7440 acc_test: 0.8080 time: 0.0342s\n",
      "Epoch: 0305 loss_train: 0.6352 acc_train: 0.8143 acc_val: 0.7420 acc_test: 0.8070 time: 0.0341s\n",
      "Epoch: 0306 loss_train: 0.6854 acc_train: 0.7714 acc_val: 0.7440 acc_test: 0.8040 time: 0.0341s\n",
      "Epoch: 0307 loss_train: 0.6133 acc_train: 0.8214 acc_val: 0.7420 acc_test: 0.7980 time: 0.0352s\n",
      "Epoch: 0308 loss_train: 0.6721 acc_train: 0.7929 acc_val: 0.7400 acc_test: 0.7980 time: 0.0341s\n",
      "Epoch: 0309 loss_train: 0.6024 acc_train: 0.7857 acc_val: 0.7420 acc_test: 0.7980 time: 0.0326s\n",
      "Epoch: 0310 loss_train: 0.6967 acc_train: 0.7643 acc_val: 0.7480 acc_test: 0.8020 time: 0.0340s\n",
      "Epoch: 0311 loss_train: 0.6100 acc_train: 0.8143 acc_val: 0.7520 acc_test: 0.8100 time: 0.0342s\n",
      "Epoch: 0312 loss_train: 0.6862 acc_train: 0.7857 acc_val: 0.7560 acc_test: 0.8160 time: 0.0342s\n",
      "Epoch: 0313 loss_train: 0.6626 acc_train: 0.7714 acc_val: 0.7640 acc_test: 0.8190 time: 0.0368s\n",
      "Epoch: 0314 loss_train: 0.6938 acc_train: 0.7643 acc_val: 0.7660 acc_test: 0.8170 time: 0.0335s\n",
      "Epoch: 0315 loss_train: 0.6002 acc_train: 0.7643 acc_val: 0.7720 acc_test: 0.8160 time: 0.0334s\n",
      "Epoch: 0316 loss_train: 0.6461 acc_train: 0.7857 acc_val: 0.7780 acc_test: 0.8160 time: 0.0343s\n",
      "Epoch: 0317 loss_train: 0.6017 acc_train: 0.8286 acc_val: 0.7740 acc_test: 0.8170 time: 0.0351s\n",
      "Epoch: 0318 loss_train: 0.5930 acc_train: 0.8071 acc_val: 0.7680 acc_test: 0.8140 time: 0.0332s\n",
      "Epoch: 0319 loss_train: 0.7490 acc_train: 0.7857 acc_val: 0.7760 acc_test: 0.8180 time: 0.0354s\n",
      "Epoch: 0320 loss_train: 0.5986 acc_train: 0.8143 acc_val: 0.7760 acc_test: 0.8140 time: 0.0333s\n",
      "Epoch: 0321 loss_train: 0.5363 acc_train: 0.8286 acc_val: 0.7800 acc_test: 0.8120 time: 0.0331s\n",
      "Epoch: 0322 loss_train: 0.6667 acc_train: 0.7786 acc_val: 0.7840 acc_test: 0.8070 time: 0.0341s\n",
      "Epoch: 0323 loss_train: 0.5707 acc_train: 0.8071 acc_val: 0.7780 acc_test: 0.8080 time: 0.0342s\n",
      "Epoch: 0324 loss_train: 0.6039 acc_train: 0.8500 acc_val: 0.7880 acc_test: 0.8060 time: 0.0342s\n",
      "Epoch: 0325 loss_train: 0.6649 acc_train: 0.8071 acc_val: 0.7780 acc_test: 0.7980 time: 0.0351s\n",
      "Epoch: 0326 loss_train: 0.5885 acc_train: 0.8286 acc_val: 0.7740 acc_test: 0.7930 time: 0.0342s\n",
      "Epoch: 0327 loss_train: 0.5522 acc_train: 0.8286 acc_val: 0.7780 acc_test: 0.7880 time: 0.0342s\n",
      "Epoch: 0328 loss_train: 0.5854 acc_train: 0.8071 acc_val: 0.7700 acc_test: 0.7810 time: 0.0339s\n",
      "Epoch: 0329 loss_train: 0.5483 acc_train: 0.8214 acc_val: 0.7660 acc_test: 0.7760 time: 0.0340s\n",
      "Epoch: 0330 loss_train: 0.4535 acc_train: 0.8500 acc_val: 0.7680 acc_test: 0.7730 time: 0.0342s\n",
      "Epoch: 0331 loss_train: 0.6717 acc_train: 0.7643 acc_val: 0.7640 acc_test: 0.7730 time: 0.0338s\n",
      "Epoch: 0332 loss_train: 0.5544 acc_train: 0.8357 acc_val: 0.7680 acc_test: 0.7780 time: 0.0341s\n",
      "Epoch: 0333 loss_train: 0.5522 acc_train: 0.8143 acc_val: 0.7680 acc_test: 0.7810 time: 0.0342s\n",
      "Epoch: 0334 loss_train: 0.5335 acc_train: 0.7857 acc_val: 0.7720 acc_test: 0.7830 time: 0.0341s\n",
      "Epoch: 0335 loss_train: 0.5523 acc_train: 0.8000 acc_val: 0.7720 acc_test: 0.7880 time: 0.0352s\n",
      "Epoch: 0336 loss_train: 0.5270 acc_train: 0.8286 acc_val: 0.7780 acc_test: 0.7900 time: 0.0334s\n",
      "Epoch: 0337 loss_train: 0.5599 acc_train: 0.8000 acc_val: 0.7700 acc_test: 0.7980 time: 0.0359s\n",
      "Epoch: 0338 loss_train: 0.5933 acc_train: 0.7857 acc_val: 0.7660 acc_test: 0.7980 time: 0.0322s\n",
      "Epoch: 0339 loss_train: 0.4627 acc_train: 0.8571 acc_val: 0.7640 acc_test: 0.7980 time: 0.0350s\n",
      "Epoch: 0340 loss_train: 0.5393 acc_train: 0.8000 acc_val: 0.7700 acc_test: 0.7980 time: 0.0328s\n",
      "Epoch: 0341 loss_train: 0.4657 acc_train: 0.8357 acc_val: 0.7620 acc_test: 0.7970 time: 0.0342s\n",
      "Epoch: 0342 loss_train: 0.5958 acc_train: 0.7714 acc_val: 0.7580 acc_test: 0.7990 time: 0.0339s\n",
      "Epoch: 0343 loss_train: 0.6606 acc_train: 0.7857 acc_val: 0.7540 acc_test: 0.7950 time: 0.0351s\n",
      "Epoch: 0344 loss_train: 0.5195 acc_train: 0.8286 acc_val: 0.7580 acc_test: 0.7940 time: 0.0334s\n",
      "Epoch: 0345 loss_train: 0.5210 acc_train: 0.8000 acc_val: 0.7540 acc_test: 0.7910 time: 0.0341s\n",
      "Epoch: 0346 loss_train: 0.5399 acc_train: 0.8357 acc_val: 0.7560 acc_test: 0.7900 time: 0.0341s\n",
      "Epoch: 0347 loss_train: 0.4963 acc_train: 0.8286 acc_val: 0.7560 acc_test: 0.7930 time: 0.0342s\n",
      "Epoch: 0348 loss_train: 0.6081 acc_train: 0.7929 acc_val: 0.7580 acc_test: 0.7910 time: 0.0342s\n",
      "Epoch: 0349 loss_train: 0.5883 acc_train: 0.7929 acc_val: 0.7680 acc_test: 0.7930 time: 0.0356s\n",
      "Epoch: 0350 loss_train: 0.6876 acc_train: 0.8214 acc_val: 0.7740 acc_test: 0.8000 time: 0.0329s\n",
      "Epoch: 0351 loss_train: 0.6265 acc_train: 0.7571 acc_val: 0.7800 acc_test: 0.8030 time: 0.0342s\n",
      "Epoch: 0352 loss_train: 0.5200 acc_train: 0.8429 acc_val: 0.7780 acc_test: 0.8000 time: 0.0343s\n",
      "Epoch: 0353 loss_train: 0.5665 acc_train: 0.7857 acc_val: 0.7720 acc_test: 0.7920 time: 0.0342s\n",
      "Epoch: 0354 loss_train: 0.6317 acc_train: 0.8000 acc_val: 0.7720 acc_test: 0.7930 time: 0.0341s\n",
      "Epoch: 0355 loss_train: 0.5915 acc_train: 0.8214 acc_val: 0.7720 acc_test: 0.7980 time: 0.0351s\n",
      "Epoch: 0356 loss_train: 0.6737 acc_train: 0.8214 acc_val: 0.7800 acc_test: 0.8040 time: 0.0330s\n",
      "Epoch: 0357 loss_train: 0.6656 acc_train: 0.8000 acc_val: 0.7800 acc_test: 0.8030 time: 0.0342s\n",
      "Epoch: 0358 loss_train: 0.6422 acc_train: 0.8143 acc_val: 0.7760 acc_test: 0.7960 time: 0.0347s\n",
      "Epoch: 0359 loss_train: 0.5315 acc_train: 0.8429 acc_val: 0.7780 acc_test: 0.7900 time: 0.0350s\n",
      "Epoch: 0360 loss_train: 0.5742 acc_train: 0.8429 acc_val: 0.7580 acc_test: 0.7760 time: 0.0325s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0361 loss_train: 0.5978 acc_train: 0.7929 acc_val: 0.7500 acc_test: 0.7700 time: 0.0356s\n",
      "Epoch: 0362 loss_train: 0.6344 acc_train: 0.7643 acc_val: 0.7460 acc_test: 0.7660 time: 0.0344s\n",
      "Epoch: 0363 loss_train: 0.5226 acc_train: 0.8571 acc_val: 0.7500 acc_test: 0.7680 time: 0.0341s\n",
      "Epoch: 0364 loss_train: 0.7077 acc_train: 0.8286 acc_val: 0.7520 acc_test: 0.7690 time: 0.0342s\n",
      "Epoch: 0365 loss_train: 0.5410 acc_train: 0.8500 acc_val: 0.7540 acc_test: 0.7740 time: 0.0343s\n",
      "Epoch: 0366 loss_train: 0.5321 acc_train: 0.8500 acc_val: 0.7580 acc_test: 0.7800 time: 0.0342s\n",
      "Epoch: 0367 loss_train: 0.5917 acc_train: 0.8143 acc_val: 0.7700 acc_test: 0.7820 time: 0.0338s\n",
      "Epoch: 0368 loss_train: 0.6703 acc_train: 0.8214 acc_val: 0.7720 acc_test: 0.7910 time: 0.0343s\n",
      "Epoch: 0369 loss_train: 0.5338 acc_train: 0.8143 acc_val: 0.7720 acc_test: 0.7900 time: 0.0343s\n",
      "Epoch: 0370 loss_train: 0.6616 acc_train: 0.8000 acc_val: 0.7720 acc_test: 0.7890 time: 0.0349s\n",
      "Epoch: 0371 loss_train: 0.5709 acc_train: 0.7857 acc_val: 0.7680 acc_test: 0.7940 time: 0.0344s\n",
      "Epoch: 0372 loss_train: 0.6557 acc_train: 0.8143 acc_val: 0.7760 acc_test: 0.7960 time: 0.0337s\n",
      "Epoch: 0373 loss_train: 0.4942 acc_train: 0.8429 acc_val: 0.7800 acc_test: 0.7960 time: 0.0341s\n",
      "Epoch: 0374 loss_train: 0.6312 acc_train: 0.8214 acc_val: 0.7880 acc_test: 0.8030 time: 0.0342s\n",
      "Epoch: 0375 loss_train: 0.5916 acc_train: 0.8214 acc_val: 0.7920 acc_test: 0.8060 time: 0.0340s\n",
      "Epoch: 0376 loss_train: 0.5411 acc_train: 0.7857 acc_val: 0.7800 acc_test: 0.8040 time: 0.0341s\n",
      "Epoch: 0377 loss_train: 0.4816 acc_train: 0.8929 acc_val: 0.7660 acc_test: 0.8030 time: 0.0343s\n",
      "Epoch: 0378 loss_train: 0.4488 acc_train: 0.8571 acc_val: 0.7600 acc_test: 0.7980 time: 0.0340s\n",
      "Epoch: 0379 loss_train: 0.4558 acc_train: 0.8643 acc_val: 0.7540 acc_test: 0.7880 time: 0.0344s\n",
      "Epoch: 0380 loss_train: 0.4753 acc_train: 0.8214 acc_val: 0.7500 acc_test: 0.7860 time: 0.0342s\n",
      "Epoch: 0381 loss_train: 0.5771 acc_train: 0.8429 acc_val: 0.7460 acc_test: 0.7830 time: 0.0339s\n",
      "Epoch: 0382 loss_train: 0.5297 acc_train: 0.8357 acc_val: 0.7440 acc_test: 0.7840 time: 0.0339s\n",
      "Epoch: 0383 loss_train: 0.5092 acc_train: 0.8071 acc_val: 0.7480 acc_test: 0.7850 time: 0.0342s\n",
      "Epoch: 0384 loss_train: 0.5909 acc_train: 0.8429 acc_val: 0.7460 acc_test: 0.7930 time: 0.0342s\n",
      "Epoch: 0385 loss_train: 0.6528 acc_train: 0.7786 acc_val: 0.7580 acc_test: 0.8040 time: 0.0356s\n",
      "Epoch: 0386 loss_train: 0.5490 acc_train: 0.8429 acc_val: 0.7540 acc_test: 0.8110 time: 0.0341s\n",
      "Epoch: 0387 loss_train: 0.8197 acc_train: 0.8143 acc_val: 0.7640 acc_test: 0.8130 time: 0.0340s\n",
      "Epoch: 0388 loss_train: 0.5486 acc_train: 0.8357 acc_val: 0.7640 acc_test: 0.8130 time: 0.0340s\n",
      "Epoch: 0389 loss_train: 0.6110 acc_train: 0.8071 acc_val: 0.7660 acc_test: 0.8110 time: 0.0341s\n",
      "Epoch: 0390 loss_train: 0.5263 acc_train: 0.8357 acc_val: 0.7720 acc_test: 0.8090 time: 0.0349s\n",
      "Epoch: 0391 loss_train: 0.5312 acc_train: 0.8214 acc_val: 0.7620 acc_test: 0.8070 time: 0.0337s\n",
      "Epoch: 0392 loss_train: 0.5518 acc_train: 0.8714 acc_val: 0.7460 acc_test: 0.8000 time: 0.0342s\n",
      "Epoch: 0393 loss_train: 0.5792 acc_train: 0.8000 acc_val: 0.7360 acc_test: 0.7920 time: 0.0348s\n",
      "Epoch: 0394 loss_train: 0.5318 acc_train: 0.8571 acc_val: 0.7340 acc_test: 0.7790 time: 0.0343s\n",
      "Epoch: 0395 loss_train: 0.5928 acc_train: 0.8000 acc_val: 0.7320 acc_test: 0.7770 time: 0.0342s\n",
      "Epoch: 0396 loss_train: 0.4458 acc_train: 0.8357 acc_val: 0.7400 acc_test: 0.7750 time: 0.0321s\n",
      "Epoch: 0397 loss_train: 0.6932 acc_train: 0.8429 acc_val: 0.7380 acc_test: 0.7690 time: 0.0349s\n",
      "Epoch: 0398 loss_train: 0.4782 acc_train: 0.8429 acc_val: 0.7320 acc_test: 0.7590 time: 0.0343s\n",
      "Epoch: 0399 loss_train: 0.5776 acc_train: 0.8214 acc_val: 0.7320 acc_test: 0.7600 time: 0.0343s\n",
      "Epoch: 0400 loss_train: 0.5411 acc_train: 0.8000 acc_val: 0.7320 acc_test: 0.7600 time: 0.0342s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 14.8022s\n",
      "0.8160000000000001 0.841\n",
      "0.8140000000000001 0.837\n",
      "0.812 0.8200000000000001\n",
      "0.81 0.839\n",
      "0.808 0.838\n",
      "0.806 0.8250000000000001\n",
      "0.804 0.8290000000000001\n",
      "0.802 0.84\n",
      "0.8 0.836\n",
      "0.798 0.8170000000000001\n"
     ]
    }
   ],
   "source": [
    "model = GCN(nfeat=features.shape[1],\n",
    "                nhid1=32,\n",
    "                nhid2=32,\n",
    "                nhid3=32,\n",
    "                nhid4=32,\n",
    "                nhid5=32,\n",
    "                nclass=labels.max().item() + 1,\n",
    "                dropout=0.8)\n",
    "model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=0.02, weight_decay=5e-4)\n",
    "t_total = time.time()\n",
    "record = {}\n",
    "for epoch in range(400):  \n",
    "    train(epoch,model,record)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "bit_list = sorted(record.keys())\n",
    "bit_list.reverse()\n",
    "for key in bit_list[:10]:\n",
    "    value = record[key]\n",
    "    print(key,value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
